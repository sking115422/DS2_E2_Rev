{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referenced Textbook: https://cobweb.cs.uga.edu/~jam/scalation_guide/comp_data_science.pdf\n",
    "\n",
    "Specifically Chapter 10 - Section 4 (Starting on page 319)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Layer Neural Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now that we understand what a percptrons. Lets complicate things a little more by now expanding to 2 layers. Show below is the basic 2L NN structure\n",
    "\n",
    "![](../pics/2l_nn/2l_nn.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, lets turn this into a model equation.\n",
    "\n",
    "we need to start buy summing the dot product of each of the b vectors with the x vectors. Then apply some function (activation function in this case) and add that our error. This is shown more concretely below:\n",
    "\n",
    "![](../pics/2l_nn/mod_eqn1.png)\n",
    "\n",
    "Now, instead of looking as this at the vector level lets condense the above expression down to the matrix level. \n",
    "\n",
    "![](../pics/2l_nn/mod_eqn2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, things are very similar to what we did with perceptron. Only real difference is the now instead of working with vectors we are working with matrices. \n",
    "\n",
    "Predicted Value Matrix:\n",
    "\n",
    "![](../pics/2l_nn/pred.png)\n",
    "\n",
    "Negative Error Matrix\n",
    "\n",
    "![](../pics/2l_nn/neg_err.png)\n",
    "\n",
    "Delta Matrix\n",
    "\n",
    "![](../pics/2l_nn/delta.png)   \n",
    "\n",
    "Below you will notice a strange symbol of a dot surrouned by a circle. This is donoting what is called the Hadamard product. The Hadamard product is just a fancy way of saying element-wise multiplication. \n",
    "\n",
    "Element-wise multiplication for matrices looks like the following:\n",
    "\n",
    "![](../pics/2l_nn/had_prod.png) \n",
    "\n",
    "Parameter (Weights) Update Equation\n",
    "\n",
    "![](../pics/2l_nn/update.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Implimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras import activations\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9 data points: one x1 x2 y\n",
    "\n",
    "xy = np.matrix (\n",
    "[[1.0, 0.0, 0.0, 0.5],\n",
    "[1.0, 0.0, 0.5, 0.3],\n",
    "[1.0, 0.0, 1.0, 0.2],\n",
    "[1.0, 0.5, 0.0, 0.8],\n",
    "[1.0, 0.5, 0.5, 0.5],\n",
    "[1.0, 0.5, 1.0, 0.3],\n",
    "[1.0, 1.0, 0.0, 1.0],\n",
    "[1.0, 1.0, 0.5, 0.8],\n",
    "[1.0, 1.0, 1.0, 0.5]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking first 3 columns of xy as matrix\n",
    "X = xy[:,0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking last column of xy as array\n",
    "y = np.array(xy[:, 3])\n",
    "\n",
    "# Squaring y and saving as array\n",
    "ysq = np.array(y ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating y and ysq as columns to for Y matrix\n",
    "Y = np.concatenate((y, ysq), axis=1)\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing B matrix\n",
    "B = np.matrix (\n",
    "[[0.1, 0.1],\n",
    "[0.2, 0.1],\n",
    "[0.1, 0.1]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing NN Parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-activation matrix\n",
    "# U = np.dot(X, B)\n",
    "U = X.dot(B)\n",
    "U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted value matrix\n",
    "Y_hat = activations.sigmoid(U)\n",
    "Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative error matrix\n",
    "E = Y_hat - Y\n",
    "E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f'(U) * E -> Hadamard Product (element wise multiplication)\n",
    "# f'(U) will be depend on activation and be different for tanh\n",
    "\n",
    "# sigmoid -> y_hat(1-y_hat) \n",
    "\n",
    "# Correction matrix\n",
    "Delta = np.multiply(np.multiply(Y_hat,(1-Y_hat)),E)\n",
    "Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradients matrix\n",
    "G = np.transpose(X) * Delta\n",
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eta is learning rate\n",
    "n = 1\n",
    "\n",
    "# Updated values matrix\n",
    "B = B - G \n",
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting 2L NN In Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting B as new variable so it will not over written in loop\n",
    "new_params = B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to hold values for plotting\n",
    "x_list = []\n",
    "sse_list = []\n",
    "rsq_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop version for calulating SST\n",
    "\n",
    "sst = 0\n",
    "for column in Y.T:\n",
    "    # print(np.mean(column))\n",
    "    for each in column:\n",
    "        sst = sst + (each - np.mean(column)) ** 2\n",
    "        \n",
    "sst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Condensed version of SST\n",
    "\n",
    "sst = np.sum(np.subtract(Y, np.mean(Y, axis=0))**2)\n",
    "\n",
    "sst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20 epochs for 2L NN\n",
    "\n",
    "for i in range(0, 25):\n",
    "    \n",
    "    # Pre-activation matrix\n",
    "    U = np.dot(X, new_params)\n",
    "    \n",
    "    # Predicted value matrix\n",
    "    Y_hat = activations.sigmoid(U)\n",
    "    \n",
    "    # Negative error matrix\n",
    "    E = Y_hat - Y\n",
    "    \n",
    "    # Correction matrix\n",
    "    Delta = np.multiply(np.multiply(Y_hat,(1-Y_hat)),E)\n",
    "    \n",
    "    # Gradients matrix\n",
    "    G = np.transpose(X) * Delta\n",
    "\n",
    "    # Eta preliminarily set to 1\n",
    "    n = 1\n",
    "    \n",
    "    # Updated values matrix\n",
    "    new_params = new_params - G \n",
    "    \n",
    "    # Sum of squared errors\n",
    "    sse = (np.linalg.norm(E)) ** 2\n",
    "    # sse = np.sum(E ** 2)\n",
    "\n",
    "    x_list.append(i)\n",
    "    sse_list.append(sse)\n",
    "    rsq_list.append(1-sse/sst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting our findings\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x_list, sse_list)\n",
    "plt.title('SSE vs Iterations')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('SSE')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x_list, rsq_list)\n",
    "plt.title(\"R^2 vs Iterations\")\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('R^2')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8338625309694086bca43e95af1b4ba2228479321112d6e6d2d9c087c2893b51"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
