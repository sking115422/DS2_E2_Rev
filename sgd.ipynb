{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization in Data Science Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referenced Textbook: https://cobweb.cs.uga.edu/~jam/scalation_guide/comp_data_science.pdf\n",
    "\n",
    "Specifically Appendix A (Starting Page 619)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic means \"randomly determined; having a random probability distribution or pattern that may be analyzed statistically but may not be predicted precisely.\"\n",
    "\n",
    "So we can guess that with STOCHASTIC graditent descent it will involve taking random samples. Indeed this is what we are doing. Now, we introduced the idea of mini-batches to the gradient decent algorithm. \n",
    "\n",
    "This is a core algorithm for data science and as such I figured we would graduate a bit from our previous example for gradient decent and focus on a data science related problem. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
